{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f86bc499",
   "metadata": {},
   "source": [
    "## download ViCILP weights and put its pth file in viclip folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e7a90379-d9ee-45d9-9073-7ed5132fa6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import polars as pl\n",
    "import torch\n",
    "from ipywidgets import HTML, GridspecLayout, Label, VBox, Video\n",
    "\n",
    "from viclip import (\n",
    "    _frame_from_video,\n",
    "    frames2tensor,\n",
    "    get_text_feat_dict,\n",
    "    get_viclip,\n",
    "    get_vid_feat,\n",
    "    retrieve_text,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4a9ecb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = get_viclip(\"l\", \"models/ViCLIP-L_InternVid-FLT-10M.pth\")\n",
    "\n",
    "data = pl.DataFrame({\"path\": [str(p) for p in Path(\"videos\").glob(\"*.mp4\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "d497aca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames(video_path):\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    return [x for x in _frame_from_video(video)]\n",
    "\n",
    "\n",
    "def probs(video_path, model=MODEL):\n",
    "    frames = get_frames(video_path)\n",
    "\n",
    "    labels = {\n",
    "        \"A stick model of a dog actively running in grass\": \"running\",\n",
    "        \"A stick model of a dog actively running in grass.\": \"running.\",\n",
    "        # \"A simulated stick model of a dog actively running towards the right in grass\": \"running-right\",\n",
    "        # \"A simulated stick model of a dog actively running towards the left in grass\": \"running-left\",\n",
    "        # \"A simulated stick model of a dog standing still in grass\": \"standing\",\n",
    "        \"A stick model of a dog doing weird things in grass\": \"weird things\",\n",
    "        \"A stick model of a dog doing weird things in grass.\": \"weird things.\",\n",
    "        \"A stick model of a dog trying to move in grass but failing\": \"failing\",\n",
    "        \"A stick model of a dog trying to move in grass but failing.\": \"failing.\",\n",
    "    }\n",
    "    texts, probs = retrieve_text(frames, list(labels.keys()), model, topk=len(labels))\n",
    "\n",
    "    result = []\n",
    "    for t, p in zip(texts, probs):\n",
    "        result.append(f\"[{p:.2f}]: {labels[t]}\")\n",
    "    return result\n",
    "\n",
    "\n",
    "def similarity_score(video_path, model=MODEL):\n",
    "    labels = {\n",
    "        \"A stick model of a dog actively running in grass\": \"running\",\n",
    "        \"A stick model of a dog actively running in grass.\": \"running.\",\n",
    "        # \"A simulated stick model of a dog actively running towards the right in grass\": \"running-right\",\n",
    "        # \"A simulated stick model of a dog actively running towards the left in grass\": \"running-left\",\n",
    "        # \"A simulated stick model of a dog standing still in grass\": \"standing\",\n",
    "        \"A stick model of a dog doing weird things in grass\": \"weird things\",\n",
    "        \"A stick model of a dog doing weird things in grass.\": \"weird things.\",\n",
    "        \"A stick model of a dog trying to move in grass but failing\": \"failing\",\n",
    "        \"A stick model of a dog trying to move in grass but failing.\": \"failing.\",\n",
    "    }\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "    frames = frames = get_frames(video_path)\n",
    "\n",
    "    clip, tokenizer = model[\"viclip\"], model[\"tokenizer\"]\n",
    "    clip = clip.to(device)\n",
    "\n",
    "    results = []\n",
    "    for label, desc in labels.items():\n",
    "        v = get_vid_feat(frames2tensor(frames, device=device), clip)\n",
    "        t = get_text_feat_dict([label], clip, tokenizer)[label]\n",
    "\n",
    "        val = torch.nn.functional.cosine_similarity(v, t).item()\n",
    "        results.append((val, desc))\n",
    "    return [f\"[sim {val:.2f}]: {desc}\" for val, desc in sorted(results, reverse=True)]\n",
    "\n",
    "\n",
    "def projection_score(video_path, model=MODEL):\n",
    "    device = torch.device(\"cuda\")\n",
    "    frames = get_frames(video_path)\n",
    "\n",
    "    clip, tokenizer = model[\"viclip\"], model[\"tokenizer\"]\n",
    "    clip = clip.to(device)\n",
    "\n",
    "    v_0 = get_vid_feat(frames2tensor(frames[:1] * 8, device=device), clip)\n",
    "    v_f = get_vid_feat(\n",
    "        frames2tensor(\n",
    "            get_frames(\"videos/4059f863-279e-41dc-8b34-48422b64c832.mp4\"), device=device\n",
    "        ),\n",
    "        clip,\n",
    "    )\n",
    "\n",
    "    v = get_vid_feat(frames2tensor(frames, device=device), clip)\n",
    "\n",
    "    labels = [\n",
    "        \"A stick model of a dog standing completely still.\",\n",
    "        \"A stick model of a dog actively running in grass.\",\n",
    "    ]\n",
    "    ts = get_text_feat_dict(labels, clip, tokenizer)\n",
    "\n",
    "    direction = v_f - v_0\n",
    "    direction = direction / direction.norm()\n",
    "    return [f\"running dir: {(v @ direction.T).item():.4f}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "8e605148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_widget(path):\n",
    "    video_html = f\"\"\"\n",
    "    <video width=\"180\" autoplay muted>\n",
    "      <source src=\"{path}\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\"\n",
    "    return HTML(video_html)\n",
    "\n",
    "\n",
    "def display_table(data, funcs):\n",
    "    grid = GridspecLayout(len(data), len(funcs) + 1)\n",
    "\n",
    "    for i, row in enumerate(data.iter_rows(named=True)):\n",
    "        video = video_widget(row[\"path\"])\n",
    "        grid[i, 0] = video\n",
    "        for j, f in enumerate(funcs, start=1):\n",
    "            grid[i, j] = VBox([Label(v, width=400) for v in f(row[\"path\"])])\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "3f931ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/InternVideo/Data/InternVid/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/InternVideo/Data/InternVid/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11df564348af4fda805681520f42f28c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridspecLayout(children=(HTML(value='\\n    <video width=\"180\" autoplay muted>\\n      <source src=\"videos/1eb2bâ€¦"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_table(data, [probs, similarity_score, projection_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cc03bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
